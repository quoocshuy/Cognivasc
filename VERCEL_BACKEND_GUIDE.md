# üöÄ Deploy Backend tr√™n Vercel

## ‚ö†Ô∏è **H·∫°n Ch·∫ø Vercel cho Backend:**

### **‚ùå V·∫•n ƒë·ªÅ:**
- **10s execution limit** - Model AI c√≥ th·ªÉ c·∫ßn nhi·ªÅu th·ªùi gian h∆°n
- **50MB function size limit** - Model 4.8MB + dependencies c√≥ th·ªÉ v∆∞·ª£t qu√°
- **Serverless functions** - Kh√¥ng ph√π h·ª£p v·ªõi persistent model loading
- **Cold start** - Model ph·∫£i load l·∫°i m·ªói l·∫ßn request

### **‚úÖ C√≥ th·ªÉ l√†m ƒë∆∞·ª£c nh∆∞ng c·∫ßn t·ªëi ∆∞u:**

---

## üõ†Ô∏è **C·∫•u H√¨nh Vercel Backend:**

### **1. Requirements (vercel-requirements.txt)**
```txt
fastapi
uvicorn[standard]
tensorflow-cpu
pillow
numpy
opencv-python-headless
python-multipart
```

### **2. API Handler (api/vercel_handler.py)**
- Serverless function handler
- X·ª≠ l√Ω CORS
- Route requests ƒë·∫øn FastAPI app

### **3. Vercel Config (vercel.json)**
```json
{
  "version": 2,
  "builds": [
    {
      "src": "api/vercel_handler.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "/api/vercel_handler.py"
    }
  ],
  "functions": {
    "api/vercel_handler.py": {
      "maxDuration": 10
    }
  }
}
```

---

## üöÄ **H∆∞·ªõng D·∫´n Deploy:**

### **B∆∞·ªõc 1: Chu·∫©n b·ªã**
```bash
# ƒê·∫£m b·∫£o c√≥ c√°c files:
# - backend/vercel-requirements.txt
# - backend/api/vercel_handler.py
# - backend/vercel.json
```

### **B∆∞·ªõc 2: Deploy Backend**
1. Truy c·∫≠p [vercel.com](https://vercel.com)
2. Click "New Project"
3. Import GitHub repository
4. **Root Directory**: `backend`
5. **Framework Preset**: Other
6. **Build Command**: `pip install -r vercel-requirements.txt`
7. **Output Directory**: (ƒë·ªÉ tr·ªëng)

### **B∆∞·ªõc 3: Environment Variables**
```env
PYTHON_VERSION=3.11
```

### **B∆∞·ªõc 4: Deploy Frontend**
1. **Root Directory**: `frontend`
2. **Build Command**: `npm install && npm run build`
3. **Output Directory**: `dist`
4. **Environment Variable**: `VITE_API_URL=https://your-backend-url.vercel.app`

---

## üîß **T·ªëi ∆Øu cho Vercel:**

### **1. Model Optimization**
```python
# Lazy loading model
model = None

def get_model():
    global model
    if model is None:
        model = tf.keras.models.load_model(MODEL_PATH)
    return model
```

### **2. Memory Management**
```python
# Clear cache after prediction
import gc
gc.collect()
```

### **3. Timeout Handling**
```python
# Set timeout cho prediction
import signal

def timeout_handler(signum, frame):
    raise TimeoutError("Prediction timeout")

signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(8)  # 8 seconds timeout
```

---

## üìä **So S√°nh Platforms:**

| Platform | Execution Time | Memory | Model Size | Best For |
|----------|----------------|--------|------------|----------|
| **Vercel** | 10s | 1GB | <50MB | Simple APIs |
| **Render** | Unlimited | 512MB | Unlimited | AI Models |
| **Fly.io** | Unlimited | 256MB | Unlimited | Performance |
| **Netlify** | 10s | 1GB | <50MB | Simple Functions |

---

## üéØ **Khuy·∫øn Ngh·ªã:**

### **‚úÖ S·ª≠ d·ª•ng Vercel n·∫øu:**
- Model nh·ªè (<10MB)
- Prediction nhanh (<5s)
- √çt requests
- Demo/Testing

### **‚ùå Kh√¥ng n√™n d√πng Vercel n·∫øu:**
- Model l·ªõn (>20MB)
- Prediction ch·∫≠m (>5s)
- Nhi·ªÅu requests
- Production

---

## üöÄ **Alternative Solutions:**

### **1. Render (Khuy·∫øn ngh·ªã)**
```bash
# Deploy backend tr√™n Render
# Kh√¥ng gi·ªõi h·∫°n execution time
# H·ªó tr·ª£ model l·ªõn
```

### **2. Fly.io**
```bash
# Deploy backend tr√™n Fly.io
# Performance cao
# Global deployment
```

### **3. Netlify Functions**
```bash
# Deploy backend tr√™n Netlify
# T∆∞∆°ng t·ª± Vercel
# 10s execution limit
```

---

## üîç **Testing Vercel Backend:**

### **1. Health Check**
```bash
curl https://your-backend-url.vercel.app/api/health
```

### **2. Prediction Test**
```bash
curl -X POST https://your-backend-url.vercel.app/api/predict \
  -H "Content-Type: application/json" \
  -d '{"image": "base64_encoded_image"}'
```

---

## ‚ö†Ô∏è **Troubleshooting:**

### **L·ªói 1: Function timeout**
```bash
# Gi·∫£m model size
# T·ªëi ∆∞u prediction code
# S·ª≠ d·ª•ng model nh·∫π h∆°n
```

### **L·ªói 2: Function size limit**
```bash
# S·ª≠ d·ª•ng tensorflow-cpu
# Lo·∫°i b·ªè dependencies kh√¥ng c·∫ßn thi·∫øt
# Compress model
```

### **L·ªói 3: Cold start ch·∫≠m**
```bash
# Pre-warm functions
# S·ª≠ d·ª•ng edge functions
# Cache model
```

---

## üéâ **K·∫øt Lu·∫≠n:**

**Vercel c√≥ th·ªÉ deploy backend nh∆∞ng kh√¥ng ph√π h·ª£p cho AI models l·ªõn.**

**Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng Render ho·∫∑c Fly.io cho backend, Vercel cho frontend.**

**Happy Deploying! üöÄ**
