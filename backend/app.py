from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import tensorflow as tf
import numpy as np
import cv2
from PIL import Image
import io, os
import logging
import sys
from typing import Optional
import time

# =============================================================================
# C·∫§U H√åNH MODEL
# =============================================================================
from config import (
    BASE_DIR, MODEL_PATH, IMG_SIZE, OPTIMIZED_THRESHOLD, CLASS_NAMES,
    HOST, PORT, WORKERS, LOG_LEVEL, LOG_FORMAT,
    CORS_ORIGINS, CORS_ALLOW_CREDENTIALS, CORS_ALLOW_METHODS, CORS_ALLOW_HEADERS,
    MAX_FILE_SIZE, ALLOWED_EXTENSIONS, MIN_IMAGE_SIZE,
    PREDICTION_TIMEOUT, MODEL_LOAD_TIMEOUT
)

# C·∫•u h√¨nh logging ƒë·ªÉ tr√°nh l·ªói encoding
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL.upper()),
    format=LOG_FORMAT,
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Global model variable
model: Optional[tf.keras.Model] = None
model_loaded: bool = False
model_load_time: Optional[float] = None

# =============================================================================
# MODEL LOADING FUNCTIONS
# =============================================================================
def load_model() -> bool:
    """T·∫£i model v√† tr·∫£ v·ªÅ True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i."""
    global model, model_loaded, model_load_time

    logger.info("=" * 50)
    logger.info("INITIALIZING ANEMIA DETECTION MODEL")
    logger.info("=" * 50)

    start_time = time.time()

    # Ki·ªÉm tra file model t·ªìn t·∫°i
    if not os.path.exists(MODEL_PATH):
        logger.error(f"Model file not found: {MODEL_PATH}")
        logger.error(f"   Current working directory: {os.getcwd()}")
        logger.error(f"   Base directory: {BASE_DIR}")
        return False

    logger.info(f"Model file found: {MODEL_PATH}")
    logger.info(f"File size: {os.path.getsize(MODEL_PATH) / (1024*1024):.2f} MB")

    try:
        # T·∫£i model
        logger.info("Loading model...")
        model = tf.keras.models.load_model(MODEL_PATH)

        # Ki·ªÉm tra model ƒë√£ t·∫£i th√†nh c√¥ng
        if model is None:
            logger.error("Model loaded but is None")
            return False

        # Test model v·ªõi input m·∫´u
        logger.info("Testing model with sample input...")
        test_input = np.random.random((1, 224, 224, 3)).astype(np.float32)
        test_output = model.predict(test_input, verbose=0)

        if test_output is None or len(test_output) == 0:
            logger.error("Model test failed - no output")
            return False

        model_load_time = time.time() - start_time
        model_loaded = True

        logger.info("Model loaded successfully!")
        logger.info(f"Load time: {model_load_time:.2f} seconds")
        logger.info(f"Model input shape: {model.input_shape}")
        logger.info(f"Model output shape: {model.output_shape}")
        logger.info(f"Model parameters: {model.count_params():,}")
        logger.info("=" * 50)

        return True

    except Exception as e:
        logger.error(f"Failed to load model: {str(e)}")
        logger.error(f"   Error type: {type(e).__name__}")
        model = None
        model_loaded = False
        return False

def validate_model_ready() -> bool:
    """Ki·ªÉm tra model c√≥ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng kh√¥ng."""
    return model_loaded and model is not None

# =============================================================================
# X·ª¨ L√ù ·∫¢NH
# =============================================================================
def preprocess_image(img_pil):
    """Ti·ªÅn x·ª≠ l√Ω ·∫£nh PIL ƒë·∫ßu v√†o ƒë·ªÉ ph√π h·ª£p v·ªõi model."""
    img_pil = img_pil.convert("RGB")  # ‚úÖ th√™m ƒë·ªÉ th·ªëng nh·∫•t
    img_np = np.array(img_pil).astype('uint8')

    if len(img_np.shape) == 2 or img_np.shape[2] == 1:
        img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)
    if img_np.shape[2] == 4:
        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2RGB)

    img_resized = cv2.resize(img_np, IMG_SIZE)
    img_expanded = np.expand_dims(img_resized, axis=0)
    return tf.keras.applications.mobilenet_v3.preprocess_input(img_expanded)

# =============================================================================
# D·ª∞ ƒêO√ÅN
# =============================================================================
def run_prediction(img_pil):
    if model is None:
        raise ValueError("Model ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")

    processed_input = preprocess_image(img_pil)
    raw_prob = model.predict(processed_input)[0][0]  # P(non-anemia)
    anemia_score = 1 - raw_prob                      # P(anemia)

    # ‚úÖ Debug log gi·ªëng h·ªát gradio.py
    logger.info("-" * 30)
    logger.info(f"[DEBUG] raw_prob (P[non-anemia]): {raw_prob:.4f}")
    logger.info(f"[DEBUG] anemia_score (P[anemia]): {anemia_score:.4f}")
    logger.info(f"[DEBUG] Threshold: {OPTIMIZED_THRESHOLD}")

    if anemia_score > OPTIMIZED_THRESHOLD:
        label = "Nghi ng·ªù Thi·∫øu m√°u"
        advice_text = (
            "<b>K·∫øt qu·∫£: C√≥ d·∫•u hi·ªáu Thi·∫øu m√°u</b><br>"
            "Khuy√™n b·∫°n n√™n tham kh·∫£o √Ω ki·∫øn b√°c sƒ© s·ªõm."
        )
    else:
        label = "Kh√¥ng nghi ng·ªù Thi·∫øu m√°u"
        advice_text = (
            "<b>K·∫øt qu·∫£: Kh√¥ng c√≥ d·∫•u hi·ªáu Thi·∫øu m√°u</b><br>"
            "H√£y duy tr√¨ l·ªëi s·ªëng l√†nh m·∫°nh v√† kh√°m s·ª©c kh·ªèe ƒë·ªãnh k·ª≥."
        )

    logger.info(f"[DEBUG] Conclusion: {label}")
    logger.info("-" * 30)

    return {
        "label": label,
        "advice": advice_text,
        "confidence": {
            "Thi·∫øu m√°u": float(anemia_score),
            "Kh√¥ng Thi·∫øu m√°u": float(raw_prob)
        }
    }

# =============================================================================
# STARTUP/SHUTDOWN EVENTS
# =============================================================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Qu·∫£n l√Ω lifecycle c·ªßa FastAPI app."""
    # Startup
    logger.info("Starting Cognivasc Anemia Detection API...")

    # T·∫£i model khi kh·ªüi ƒë·ªông
    if not load_model():
        logger.error("CRITICAL: Model failed to load. Server will start but predictions will fail.")
        logger.error("   Please check model file and restart the server.")
    else:
        logger.info("Server ready to accept requests!")

    yield

    # Shutdown
    logger.info("Shutting down server...")
    global model
    model = None

# =============================================================================
# FASTAPI APP
# =============================================================================
app = FastAPI(
    title="Cognivasc Anemia Detection API",
    description="REST API cho h·ªá th·ªëng d·ª± ƒëo√°n thi·∫øu m√°u qua ·∫£nh k·∫øt m·∫°c",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=CORS_ALLOW_CREDENTIALS,
    allow_methods=CORS_ALLOW_METHODS,
    allow_headers=CORS_ALLOW_HEADERS,
)

# =============================================================================
# ENDPOINTS
# =============================================================================
@app.get("/")
async def root():
    """Root endpoint v·ªõi th√¥ng tin API."""
    return {
        "message": "Cognivasc Anemia Detection API",
        "version": "1.0.0",
        "status": "running",
        "model_ready": validate_model_ready(),
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint ƒë·ªÉ ki·ªÉm tra tr·∫°ng th√°i server v√† model."""
    health_status = {
        "status": "healthy" if validate_model_ready() else "unhealthy",
        "timestamp": time.time(),
        "model": {
            "loaded": model_loaded,
            "ready": validate_model_ready(),
            "load_time": model_load_time,
            "path": MODEL_PATH,
            "exists": os.path.exists(MODEL_PATH)
        },
        "server": {
            "uptime": time.time() - (model_load_time or 0),
            "version": "1.0.0"
        }
    }

    status_code = 200 if validate_model_ready() else 503
    return JSONResponse(content=health_status, status_code=status_code)

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """Endpoint d·ª± ƒëo√°n thi·∫øu m√°u t·ª´ ·∫£nh k·∫øt m·∫°c."""

    # Ki·ªÉm tra model c√≥ s·∫µn s√†ng kh√¥ng
    if not validate_model_ready():
        raise HTTPException(
            status_code=503,
            detail="Model is not ready. Please check server health at /health"
        )

    # Ki·ªÉm tra file type
    if not file.content_type or not file.content_type.startswith('image/'):
        raise HTTPException(
            status_code=400,
            detail="File must be an image"
        )

    try:
        logger.info(f"Processing image: {file.filename} ({file.content_type})")

        contents = await file.read()
        img = Image.open(io.BytesIO(contents))

        # Ki·ªÉm tra k√≠ch th∆∞·ªõc ·∫£nh
        if img.size[0] < MIN_IMAGE_SIZE[0] or img.size[1] < MIN_IMAGE_SIZE[1]:
            raise HTTPException(
                status_code=400,
                detail=f"Image too small. Minimum size: {MIN_IMAGE_SIZE[0]}x{MIN_IMAGE_SIZE[1]} pixels"
            )

        result = run_prediction(img)
        logger.info(f"Prediction completed for {file.filename}")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in prediction process: {str(e)}")
        logger.error(f"   Error type: {type(e).__name__}")
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn

    print("=" * 60)
    print("COGNIVASC ANEMIA DETECTION API")
    print("=" * 60)
    print("Server Information:")
    print(f"   - Host: {HOST}")
    print(f"   - Port: {PORT}")
    print(f"   - Model: {MODEL_PATH}")
    print(f"   - Threshold: {OPTIMIZED_THRESHOLD}")
    print("=" * 60)
    print("Available Endpoints:")
    print(f"   - Root: http://{HOST}:{PORT}/")
    print(f"   - Health: http://{HOST}:{PORT}/health")
    print(f"   - Predict: http://{HOST}:{PORT}/predict")
    print(f"   - Docs: http://{HOST}:{PORT}/docs")
    print(f"   - Redoc: http://{HOST}:{PORT}/redoc")
    print("=" * 60)
    print("Starting server...")
    print("=" * 60)

    try:
        uvicorn.run(
            app,
            host=HOST,
            port=PORT,
            log_level=LOG_LEVEL.lower(),
            access_log=True
        )
    except KeyboardInterrupt:
        print("\nServer stopped by user")
    except Exception as e:
        print(f"\nServer failed to start: {e}")
        sys.exit(1)

# =============================================================================
# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG
# =============================================================================
"""
üöÄ C√ÅCH CH·∫†Y SERVER:

1. Ch·∫°y tr·ª±c ti·∫øp:
   python app.py

2. Ch·∫°y v·ªõi uvicorn:
   uvicorn app:app --host 0.0.0.0 --port 8000 --reload

3. Ch·∫°y v·ªõi gunicorn (production):
   gunicorn app:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000

üîç KI·ªÇM TRA TR·∫†NG TH√ÅI:

1. Health check:
   curl http://localhost:8000/health

2. API documentation:
   http://localhost:8000/docs

3. Test prediction:
   curl -X POST "http://localhost:8000/predict" \
        -H "accept: application/json" \
        -H "Content-Type: multipart/form-data" \
        -F "file=@test_image.jpg"

üìù L∆ØU √ù:
- Model s·∫Ω ƒë∆∞·ª£c t·∫£i t·ª± ƒë·ªông khi kh·ªüi ƒë·ªông server
- N·∫øu model kh√¥ng t·∫£i ƒë∆∞·ª£c, server v·∫´n ch·∫°y nh∆∞ng /predict s·∫Ω tr·∫£ v·ªÅ l·ªói 503
- S·ª≠ d·ª•ng /health ƒë·ªÉ ki·ªÉm tra tr·∫°ng th√°i model
- API h·ªó tr·ª£ CORS cho frontend
"""
